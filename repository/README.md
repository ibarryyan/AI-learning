| **仓库标题**                 | **链接**                                                      | **简要描述**                                                              |
| ---------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------- |
| **HuggingFace Transformers** | [GitHub](https://github.com/huggingface/transformers)         | 提供BERT/GPT/LLaMA等主流大模型的PyTorch/TF实现，支持快速训练与推理        |
| **LangChain**                | [GitHub](https://github.com/langchain-ai/langchain)           | 构建大模型应用的开发框架，支持与OpenAI/Gemini等API集成及工具链扩展        |
| **LLaMA-Factory**            | [GitHub](https://github.com/hiyouga/LLaMA-Factory)            | 一站式大模型微调工具，支持LoRA/QLoRA等高效训练方法，兼容LLaMA/Mistral系列 |
| **Prompt Engineering Guide** | [GitHub](https://github.com/dair-ai/Prompt-Engineering-Guide) | 系统讲解Prompt设计技巧，覆盖GPT/Claude等模型的优化策略与案例              |
| **DeepSpeed**                | [GitHub](https://github.com/microsoft/DeepSpeed)              | 微软开发的分布式训练加速库，支持千亿参数模型的低成本训练与推理优化        |
| **Chinese-LLaMA-Alpaca**     | [GitHub](https://github.com/ymcui/Chinese-LLaMA-Alpaca)       | 开源中文增强版LLaMA模型，提供词表扩展与指令精调方案                       |
| **OpenAI Cookbook**          | [GitHub](https://github.com/openai/openai-cookbook)           | OpenAI官方示例库，包含API使用、微调、安全实践等场景代码                   |
| **vLLM**                     | [GitHub](https://github.com/vllm-project/vllm)                | 高性能推理框架，支持并行采样与显存优化，加速LLaMA/GPT等模型推理速度       |
| **ChatGLM-6B**               | [GitHub](https://github.com/THUDM/ChatGLM-6B)                 | 清华开源的62亿参数中英双语对话模型，支持本地部署与微调                    |
| **Awesome-LLM**              | [GitHub](https://github.com/Hannibal046/Awesome-LLM)          | 大模型资源聚合仓库，涵盖论文/教程/工具库/应用案例的持续更新列表           |
