## ğŸš¢ç¨‹åºå‘˜AIå­¦ä¹ æŒ‡å—

![Static Badge](https://img.shields.io/badge/AI%20Learning-é—«åŒå­¦-8A2BE2)
<a href="https://github.com/ibarryyan/golang-tips-100/blob/master/img/wechat.jpg"><img src="https://img.shields.io/badge/%E5%85%AC%E4%BC%97%E5%8F%B7-%E6%89%AF%E7%BC%96%E7%A8%8B%E7%9A%84%E6%B7%A1-blue" alt="å…¬ä¼—å·"></a>
[![](https://img.shields.io/github/stars/ibarryyan/AI-learning.svg?style=flat)](https://github.com/ibarryyan/AI-learning/stargazers)

æ”¶é›†å’Œæ•´ç†ä¸AIå­¦ä¹ ç›¸å…³çš„èµ„æºï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•™ç¨‹ã€ä»£ç ç¤ºä¾‹ã€è®ºæ–‡ã€ä¹¦ç±æ¨èç­‰ã€‚æ— è®ºä½ æ˜¯AIé¢†åŸŸçš„æ–°æ‰‹è¿˜æ˜¯æœ‰ä¸€å®šç»éªŒçš„å¼€å‘è€…ï¼Œéƒ½å¸Œæœ›è¿™ä¸ªä»“åº“èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚

### ğŸŸ¢å¼€æºAIå¤§æ¨¡å‹

| æ¨¡å‹            | é“¾æ¥                                                          | è¯´æ˜                              | æ¨èæ˜Ÿçº§ |
|:------------- |:----------------------------------------------------------- |:------------------------------- |:----:|
| LLaMA2        | [LLaMA2](https://github.com/facebookresearch/llama)         | Metaæ¨å‡ºçš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºç ”ç©¶ä¸åº”ç”¨ã€‚        | â­â­â­â­ |
| GPT-J         | [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax) | EleutherAIå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘GPT-3ã€‚ | â­â­â­â­ |
| Alpaca        | [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)      | åŸºäºLLaMAçš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ï¼Œé€‚ç”¨äºäº¤äº’å¼åº”ç”¨ã€‚        | â­â­â­â­ |
| OpenAssistant | [OpenAssistant](https://github.com/LAION-AI/Open-Assistant) | LAIONå¼€å‘çš„å¼€æºåŠ©æ‰‹æ¨¡å‹ï¼Œæ³¨é‡å¯¹è¯èƒ½åŠ›ã€‚          | â­â­â­â­ |
| Vicuna        | [Vicuna](https://github.com/lm-sys/FastChat)                | åŸºäºLLaMAçš„å¯¹è¯ä¼˜åŒ–æ¨¡å‹ï¼Œé€‚ç”¨äºäº¤äº’å¼åº”ç”¨ã€‚        | â­â­â­â­ |
| GPT-NeoX      | [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)          | EleutherAIå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå¤§è§„æ¨¡è®­ç»ƒã€‚   | â­â­â­â­ |

### ğŸŒAIå¤§æ¨¡å‹äº§å“

| äº§å“åç§°      | ç½‘å€                                    |
|:--------- |:------------------------------------- |
| DeepSeek  | https://chat.deepseek.com/            | 
| ChatGPT   | https://chat.openai.com/              | 
| Bard      | https://bard.google.com/              | 
| Claude    | https://www.anthropic.com/            | 
| æ–‡å¿ƒä¸€è¨€      | https://yiyan.baidu.com/              | 
| é€šä¹‰åƒé—®      | https://tongyi.aliyun.com/            | 
| è®¯é£æ˜Ÿç«      | https://xinghuo.xfyun.cn/             | 
| æ™ºè°±æ¸…è¨€      | https://chatglm.cn/                   | 
| AIæ–‡æœ¬äººæ€§åŒ–   | https://humanize.im/zh-CN             | 
| è…¾è®¯å…ƒå™¨      | https://yuanqi.tencent.com/agent-shop |
| Napkin    | https://chat01.ai/?ref=nyi8wifx       |
| Chat01.ai | https://www.napkin.ai/                | 

### ğŸ”¦æ•™ç¨‹Tutorial

| æ•™ç¨‹              | é“¾æ¥                                                            | è¯´æ˜                          | æ¨èæ˜Ÿçº§ |
|:--------------- |:------------------------------------------------------------- |:--------------------------- |:----:|
| é¢å‘åˆå­¦è€…çš„äººå·¥æ™ºèƒ½è¯¾ç¨‹    | [Link](https://github.com/microsoft/AI-For-Beginners)         | å¾®è½¯æ¨å‡ºçš„é¢å‘åˆå­¦è€…çš„äººå·¥æ™ºèƒ½è¯¾ç¨‹           | â­â­â­  |
| LLMCookbook     | [LLMCookbook](https://github.com/datawhalechina/llm-cookbook) | é¢å‘å¼€å‘è€…çš„å¤§æ¨¡å‹ç³»åˆ—æ•™ç¨‹ï¼Œæ¶µç›–APIä½¿ç”¨ä¸åº”ç”¨å¼€å‘ã€‚ | â­â­â­â­ |
| Hugging Face æ•™ç¨‹ | [Hugging Face](https://huggingface.co/learn)                  | æä¾›ä¸°å¯Œçš„NLPæ¨¡å‹ä½¿ç”¨ä¸è®­ç»ƒæ•™ç¨‹ã€‚          | â­â­â­  |
| Stanford CS224n | [CS224n](http://web.stanford.edu/class/cs224n/)               | æ–¯å¦ç¦å¤§å­¦çš„è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹ï¼Œæ¶µç›–æœ€æ–°ç ”ç©¶è¿›å±•ã€‚    | â­â­â­â­ |
| PyTorch å®˜æ–¹æ•™ç¨‹    | [PyTorch Tutorials](https://pytorch.org/tutorials/)           | å®˜æ–¹æä¾›çš„PyTorchä½¿ç”¨ä¸æ·±åº¦å­¦ä¹ æ•™ç¨‹ã€‚      | â­â­â­  |

### â›ï¸é¡¹ç›®å·¥å…·Tools

| èµ„æ–™åç§°                      | é“¾æ¥                                                   | è¯´æ˜                           | æ¨èæ˜Ÿçº§ |
|:------------------------- |:---------------------------------------------------- |:---------------------------- |:----:|
| TensorFlow                | [TensorFlow](https://www.tensorflow.org/)            | Googleå¼€å‘çš„å¼€æºæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒå¤šç§å¹³å°ã€‚    | â­â­â­â­ |
| PyTorch                   | [PyTorch](https://pytorch.org/)                      | Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œçµæ´»æ€§é«˜ï¼Œé€‚åˆç ”ç©¶ã€‚ | â­â­â­â­ |
| Hugging Face Transformers | [Transformers](https://huggingface.co/transformers/) | æä¾›ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹ä¸å·¥å…·ï¼Œä¾¿äºNLPä»»åŠ¡å¼€å‘ã€‚     | â­â­â­â­ |
| OpenCV                    | [OpenCV](https://opencv.org/)                        | è®¡ç®—æœºè§†è§‰åº“ï¼Œæä¾›ä¸°å¯Œçš„å›¾åƒå¤„ç†åŠŸèƒ½ã€‚          | â­â­â­  |

### ğŸ“œè®ºæ–‡Papers

| èµ„æ–™åç§°                                                                                  | é“¾æ¥                                                                             | è¯´æ˜                             | æ¨èæ˜Ÿçº§ |
|:------------------------------------------------------------------------------------- |:------------------------------------------------------------------------------ |:------------------------------ |:----:|
| 2024å¹´å€¼å¾—æ³¨æ„çš„äººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡ï¼ˆä¸€ï¼‰                                                                 | [Link](https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1) | 2024å¹´å€¼å¾—æ³¨æ„çš„äººå·¥æ™ºèƒ½ç ”ç©¶è®ºæ–‡ï¼ˆä¸€ï¼‰          | â­â­â­  |
| Attention Is All You Need                                                             | [è®ºæ–‡](https://arxiv.org/abs/1706.03762)                                         | æå‡ºTransformeræ¨¡å‹ï¼Œæ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚   | â­â­â­  |
| BERT: Pre-training of Deep Bidirectional Transformers                                 | [è®ºæ–‡](https://arxiv.org/abs/1810.04805)                                         | æå‡ºBERTæ¨¡å‹ï¼Œå¼€å¯äº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ–°ç¯‡ç« ã€‚       | â­â­â­  |
| GPT-3: Language Models are Few-Shot Learners                                          | [è®ºæ–‡](https://arxiv.org/abs/2005.14165)                                         | æè¿°GPT-3æ¨¡å‹çš„æ¶æ„ä¸èƒ½åŠ›ï¼Œå±•ç¤ºå…¶åœ¨å¤šä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚   | â­â­â­  |
| Vision Transformer (ViT)                                                              | [è®ºæ–‡](https://arxiv.org/abs/2010.11929)                                         | å°†Transformeræ¶æ„åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚      | â­â­   |
| YOLOv4: Optimal Speed and Accuracy                                                    | [è®ºæ–‡](https://arxiv.org/abs/2004.10934)                                         | æå‡ºYOLOv4æ¨¡å‹ï¼Œæå‡äº†ç›®æ ‡æ£€æµ‹çš„é€Ÿåº¦ä¸å‡†ç¡®æ€§ã€‚     | â­â­   |
| Reinforcement Learning with Unsupervised Auxiliary Tasks                              | [è®ºæ–‡](https://arxiv.org/abs/1611.05397)                                         | æ¢è®¨åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¼•å…¥è¾…åŠ©ä»»åŠ¡ä»¥æå‡æ€§èƒ½ã€‚           | â­â­   |
| Self-Attention Generative Adversarial Networks (SAGAN)                                | [è®ºæ–‡](https://arxiv.org/abs/1805.08318)                                         | åœ¨GANä¸­å¼•å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡ç”Ÿæˆæ•ˆæœã€‚          | â­â­   |
| CLIP: Learning Transferable Visual Models                                             | [è®ºæ–‡](https://arxiv.org/abs/2103.00020)                                         | ç»“åˆå›¾åƒä¸æ–‡æœ¬çš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œæå‡å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚      | â­â­â­  |
| AlphaFold: Using AI for Scientific Discovery                                          | [è®ºæ–‡](https://www.nature.com/articles/s41586-021-03819-2)                       | DeepMindå¼€å‘çš„è›‹ç™½è´¨ç»“æ„é¢„æµ‹æ¨¡å‹ï¼Œçªç ´æ€§æˆæœã€‚    | â­â­â­  |
| Sparse Transformer                                                                    | [è®ºæ–‡](https://arxiv.org/abs/1904.10509)                                         | æå‡ºç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡Transformerçš„æ•ˆç‡ã€‚    | â­â­â­  |
| T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [è®ºæ–‡](https://arxiv.org/abs/1910.10683)                                         | æå‡ºT5æ¨¡å‹ï¼Œå°†æ‰€æœ‰NLPä»»åŠ¡ç»Ÿä¸€ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬çš„æ¡†æ¶ã€‚    | â­â­â­  |
| Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context               | [è®ºæ–‡](https://arxiv.org/abs/1901.02860)                                         | æå‡ºTransformer-XLï¼Œæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡çš„è¯­è¨€æ¨¡å‹ã€‚ | â­â­â­  |

### ğŸ“–ä¹¦ç±æ¨èBooks

| æ•™ç¨‹  | Link | è¯´æ˜  | æ¨èæ˜Ÿçº§ | å¤‡æ³¨  |
| --- | ---- | --- | ---- | --- |
|     |      |     |   |     |

## ğŸ¤”å¦‚ä½•ä½¿ç”¨How to use

## ğŸ˜¶â€ğŸŒ«ï¸è”ç³»ä½œè€…

E-mailï¼šyanmingxin.boy@gmail.com

WeChat: æ‰¯ç¼–ç¨‹çš„æ·¡

![å›¾ç‰‡æè¿°](assets/wx.png)