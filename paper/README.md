#### DeepSeek相关


| 标题                                                                                          | 链接                                                                 | 中文标题                                            |
| --------------------------------------------------------------------------------------------- | -------------------------------------------------------------------- | --------------------------------------------------- |
| 《DeepSeek-V3 Technical Report》                                                              | [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437) | DeepSeek-V3 技术报告                                |
| 《DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model》        | [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434) | DeepSeek-V2：一个强大、经济且高效的专家混合语言模型 |
| 《DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models》 | [https://arxiv.org/abs/2401.06066](https://arxiv.org/abs/2401.06066) | DeepSeekMoE：迈向专家混合语言模型的终极专家化       |
| 《DeepSeek LLM: Scaling Open-Source Language Models with Longtermism》                        | [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954) | DeepSeek LLM：以长远主义扩展开源语言模型            |
| 《DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning》        | [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948) | DeepSeek-R1：通过强化学习激励大型语言模型的推理能力 |
